{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import sleep\n",
    "sleep(0.0416)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython import display\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import rescale\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # If frame is a tuple, extract the first element (assuming it's the image)\n",
    "    if isinstance(frame, tuple):\n",
    "        frame = frame[0]  # Adjust the index based on the structure\n",
    "    \n",
    "    # Convert to grayscale and rescale\n",
    "    gray = rgb2gray(frame)\n",
    "    scaled = rescale(gray, 0.5, anti_aliasing=True)\n",
    "    return scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3456, 512),  # Adjust input size to match flattened convolutional output\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.05  # Exploration probability\n",
    "TARGET_UPDATE = 10  # How often to update the target network\n",
    "MEMORY_CAPACITY = 10000\n",
    "\n",
    "# Setup the Atari environment\n",
    "env = gym.make('Pong-v4', render_mode=\"human\")\n",
    "n_actions = env.action_space.n\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "\n",
    "# Create networks\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_net = DQN((1, 84, 84), n_actions).to(device)\n",
    "target_net = DQN((1, 84, 84), n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Replay memory\n",
    "memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, steps_done):\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPSILON\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = target_net(next_state_batch).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = nn.SmoothL1Loss()(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def reward_function(observation, action, next_observation):\n",
    "    reward = 0\n",
    "\n",
    "    # Example reward structure based on game events\n",
    "    if next_observation['score'] > observation['score']:\n",
    "        reward += 1  # Positive reward for increasing the score\n",
    "    elif next_observation['lives'] < observation['lives']:\n",
    "        reward -= 1  # Negative reward for losing a life\n",
    "    elif action == \"shoot\" and next_observation['enemy_defeated']:\n",
    "        reward += 5  # Reward for defeating an enemy\n",
    "    \n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macenh\\Documents\\RCOS\\ReinforcementLearning\\MaxCut_RL\\tutorials\\holden_macentee\\.venv\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = preprocess_frame(state)\n",
    "    state = torch.tensor(state, device=device, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    for t in range(10000):\n",
    "        env.render()\n",
    "        action = select_action(state, t)\n",
    "        obs, reward, done, _, _= env.step(action.item())\n",
    "        if reward == 0:\n",
    "            reward = -0.1\n",
    "        #cv2.imshow(\"Game\", next_state)\n",
    "        \n",
    "        \n",
    "        next_state = preprocess_frame(obs)\n",
    "        next_state = torch.tensor(next_state, device=device, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
